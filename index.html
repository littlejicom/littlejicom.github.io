<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>littleji</title>
  <meta name="author" content="littleji">
  
  <meta name="description" content="littleji&#39;s blog">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="littleji">

  
    <meta property="og:image" content="undefined">
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="littleji" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-77530561-1', 'auto');
	ga('send', 'pageview');

</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-127734909-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-127734909-1');
</script>


</head>
</html>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet">
  <h1><a href="/">littleji</a></h1>
  <h2><a href="/">a blog</a></h2>
  <h2><a href="https://github.com/littleji" title="GithubID:littleji" target="_blank"><i class="fa fa-3x fa-github"></i></a> &nbsp;</h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2019-03-02T16:00:00.000Z"><a href="/2019/03/03/20190303DataWhaleNLPTask1/">2019-03-03</a></time>
      
      
  
    <h1 class="title"><a href="/2019/03/03/20190303DataWhaleNLPTask1/">windows下TensorFlow安装与imdb文本分类</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="windows下进行TensorFlow-GPU版本安装-不适用于CPU版本"><a href="#windows下进行TensorFlow-GPU版本安装-不适用于CPU版本" class="headerlink" title="windows下进行TensorFlow GPU版本安装(不适用于CPU版本)"></a>windows下进行TensorFlow GPU版本安装(不适用于CPU版本)</h2><p>基于各种原因，本次的实验环境在windows下进行。<br>其中本机的配置如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">硬件：</span><br><span class="line"> - CPU：Intel Xeon E3-1505M</span><br><span class="line"> - RAM：64GB</span><br><span class="line"> - GPU：NVIDIA Quadro M2000M</span><br><span class="line">软件：</span><br><span class="line"> - OS：Windows 10 专业工作站 1809 x64</span><br></pre></td></tr></table></figure></p>
<p>正确的步骤如下所示:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1. 请务必保证安装的 `Python` 版本为 `3.6.X`, `TensorFlow` 对于 `3.7.X`还未支持完全</span><br><span class="line">2. 安装 Cuda 10.0, 目前不要使用Cuda 10.1的版本,因为还未支持</span><br><span class="line">3. 安装 cudnn for 10.0 </span><br><span class="line">4. 安装 tensorflow-gpu</span><br><span class="line">5. 添加对应的环境变量</span><br><span class="line">SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;%PATH%</span><br><span class="line">SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64;%PATH%</span><br><span class="line">SET PATH=C:\tools\cuda\bin;%PATH%</span><br><span class="line">6. 验证(在输入下面的代码并运行后,会出现3min的等待,目前原因不明)</span><br><span class="line">python -c &quot;import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))&quot;</span><br><span class="line">7. 结果</span><br><span class="line">2019-03-03 17:28:53.391788: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2</span><br><span class="line">2019-03-03 17:28:53.569006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:</span><br><span class="line">name: Quadro M2000M major: 5 minor: 0 memoryClockRate(GHz): 1.137</span><br><span class="line">pciBusID: 0000:01:00.0</span><br><span class="line">totalMemory: 4.00GiB freeMemory: 3.34GiB</span><br><span class="line">2019-03-03 17:28:53.600653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0</span><br><span class="line">2019-03-03 17:28:54.903988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2019-03-03 17:28:54.921578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0</span><br><span class="line">2019-03-03 17:28:54.932974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0: N</span><br><span class="line">2019-03-03 17:28:54.946509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -&gt; physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)</span><br><span class="line">tf.Tensor(1187.2697, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure></p>
<h2 id="使用-imdb数据集进行文本分类"><a href="#使用-imdb数据集进行文本分类" class="headerlink" title="使用 imdb数据集进行文本分类"></a>使用 imdb数据集进行文本分类</h2><p>首先使用下属的代码下载数据集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imdb = keras.datasets.imdb</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)</span><br></pre></td></tr></table></figure></p>
<p>下载完之后我们可以看到该数据集包含了25000条训练数据和25000条测试数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Training entries: &#123;&#125;, labels: &#123;&#125;&quot;.format(len(train_data), len(test_data)))</span><br></pre></td></tr></table></figure></p>
<p>我们查看某一条具体的imdb影评是什么样的格式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(train_data[0])</span><br><span class="line">----</span><br><span class="line">[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]</span><br></pre></td></tr></table></figure></p>
<p>说明所有的影评根据词典已经转化为相应的数字,我们尝试将对应的数字转换为真实的词<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 构建一个词与数字的映射字典</span><br><span class="line">word_index = imdb.get_word_index()</span><br><span class="line"># 创建一个数字与词的映射字典</span><br><span class="line">word_index = &#123;k:(v+3) for k,v in word_index.items()&#125; </span><br><span class="line">word_index[&quot;&lt;PAD&gt;&quot;] = 0</span><br><span class="line">word_index[&quot;&lt;START&gt;&quot;] = 1</span><br><span class="line">word_index[&quot;&lt;UNK&gt;&quot;] = 2 # unknown</span><br><span class="line">word_index[&quot;&lt;UNUSED&gt;&quot;] = 3</span><br><span class="line"></span><br><span class="line">index_word = dict([(value, key) for (key, value) in word_index.items()])</span><br><span class="line"></span><br><span class="line">def decode_review(text):</span><br><span class="line">    return &apos; &apos;.join([index_word.get(i, &apos;?&apos;) for i in text])</span><br></pre></td></tr></table></figure></p>
<p>现在我们就可以通过<code>decode_review(train_data[0])</code>来获得原始的影评了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;&lt;START&gt; this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert &lt;UNK&gt; is an amazing actor and now the same being director &lt;UNK&gt; father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for &lt;UNK&gt; and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also &lt;UNK&gt; to the two little boy&apos;s that played the &lt;UNK&gt; of norman and paul they were just brilliant children are often left out of the &lt;UNK&gt; list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all&quot;</span><br></pre></td></tr></table></figure></p>
<p>为了更好的进行下一步的神经网络处理,我们将对应的影评数据进行向量化,向量的长度为256<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_data = keras.preprocessing.sequence.pad_sequences(train_data,</span><br><span class="line">                                                        value=word_index[&quot;&lt;PAD&gt;&quot;],</span><br><span class="line">                                                        padding=&apos;post&apos;,</span><br><span class="line">                                                        maxlen=256)</span><br><span class="line"></span><br><span class="line">test_data = keras.preprocessing.sequence.pad_sequences(test_data,</span><br><span class="line">                                                       value=word_index[&quot;&lt;PAD&gt;&quot;],</span><br><span class="line">                                                       padding=&apos;post&apos;,</span><br><span class="line">                                                       maxlen=256)</span><br></pre></td></tr></table></figure></p>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 初始化10000词的词典</span><br><span class="line">vocab_size = 10000</span><br><span class="line"></span><br><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(keras.layers.Embedding(vocab_size, 16))#每个词的向量长度为16</span><br><span class="line">model.add(keras.layers.GlobalAveragePooling1D())</span><br><span class="line">model.add(keras.layers.Dense(16, activation=tf.nn.relu))</span><br><span class="line">model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h3 id="创建优化器"><a href="#创建优化器" class="headerlink" title="创建优化器"></a>创建优化器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 使用adam算法</span><br><span class="line">model.compile(optimizer=&apos;adam&apos;,</span><br><span class="line">              loss=&apos;binary_crossentropy&apos;,</span><br><span class="line">              metrics=[&apos;acc&apos;])</span><br><span class="line"># 创建一个验证集</span><br><span class="line">x_val = train_data[:10000]</span><br><span class="line">partial_x_train = train_data[10000:]</span><br><span class="line"></span><br><span class="line">y_val = train_labels[:10000]</span><br><span class="line">partial_y_train = train_labels[10000:]</span><br></pre></td></tr></table></figure>
<h3 id="开始进行训练"><a href="#开始进行训练" class="headerlink" title="开始进行训练"></a>开始进行训练</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=40,</span><br><span class="line">                    batch_size=512,</span><br><span class="line">                    validation_data=(x_val, y_val),</span><br><span class="line">                    verbose=1)</span><br></pre></td></tr></table></figure>
<h3 id="评估对应的结果"><a href="#评估对应的结果" class="headerlink" title="评估对应的结果"></a>评估对应的结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results = model.evaluate(test_data, test_labels)</span><br><span class="line">print(results)</span><br></pre></td></tr></table></figure>
<h3 id="使用到的相关的术语"><a href="#使用到的相关的术语" class="headerlink" title="使用到的相关的术语"></a>使用到的相关的术语</h3><ol>
<li>ROC(Receiver Operating Characteristic)的操作方式与之前的P-R图类似,并提出了两个概念,真正利率TPR,假正利率FPR<script type="math/tex; mode=display">
TPR=TP/(TP+FN)</script><script type="math/tex; mode=display">
FPR=FP/(FP+TN)</script></li>
<li>ACC 精确度,即正确与全集之比</li>
<li>召回率,即正确的与集合中所有真正正确的数据集之比</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://tensorflow.google.cn/tutorials/keras/basic_text_classification" target="_blank" rel="noopener">使用keras进行文本分类</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2019/03/03/20190303DataWhaleNLPTask1/">http://littleji.com/2019/03/03/20190303DataWhaleNLPTask1/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2019-02-18T16:00:00.000Z"><a href="/2019/02/19/20190219HandoverMemoList/">2019-02-19</a></time>
      
      
  
    <h1 class="title"><a href="/2019/02/19/20190219HandoverMemoList/">软件项目交接清单</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="1-项目交接的场景"><a href="#1-项目交接的场景" class="headerlink" title="1 项目交接的场景"></a>1 项目交接的场景</h2><ul>
<li>由于同事离职，将工作交接给自己。</li>
<li>由于自己离职，将工作交接给同事。</li>
<li>由于项目变动，将工作交给其他项目组。</li>
</ul>
<h2 id="2-文档相关注意点"><a href="#2-文档相关注意点" class="headerlink" title="2 文档相关注意点"></a>2 文档相关注意点</h2><ul>
<li>确定每一期的产品需求文档，PRD文档，概要设计文档，详细设计文档（流程图、设计框架图、上下游组件交互图），接口设计文档。</li>
<li>确定开发过程中可能使用到的PSD等视觉稿或切图。</li>
<li>确定每一期的测试覆盖文档，单元测试文档，遗留Bug。</li>
<li>确定目前这一期的开发进度文档，包括尚未开发，开发过程中，开发完成。</li>
<li>确定每一期的使用说明文档。</li>
<li>确定FAQ文档。</li>
<li>确定每一期的产品人员，开发人员，测试人员。</li>
<li>确定每一期的开始开发时间，提测时间，上线时间。</li>
<li>确定之前所编写的专利、软件著作权等文档。</li>
<li>了解相关程序风险，遗留问题等。</li>
<li>其他的一些约定如Scrum、XP、瀑布等软件开发方法。</li>
</ul>
<h2 id="3-项目源码相关注意点"><a href="#3-项目源码相关注意点" class="headerlink" title="3 项目源码相关注意点"></a>3 项目源码相关注意点</h2><ul>
<li>将开发人员最后修改的代码提交。</li>
<li>确定项目源码且需要有规范的详细注释。</li>
<li>确定源码目录结构说明，明确结构中各个部分的意义。</li>
<li>确定项目对外API。</li>
<li>确定项目定时脚本。</li>
<li>确定项目日志查看平台。</li>
<li>开通SVN的权限。</li>
<li>开通GIT的权限。</li>
<li>开通涉及交接的任何系统权限。</li>
<li>了解上线部署脚本、流程。</li>
<li>了解代码规范。</li>
</ul>
<h2 id="4-数据库相关注意点"><a href="#4-数据库相关注意点" class="headerlink" title="4 数据库相关注意点"></a>4 数据库相关注意点</h2><ul>
<li>相关数据库与数据表结构</li>
<li>查看是否有未注释的库名、表名、字段名，将其确定。</li>
<li>最好了解每一个数据库、表、字段的意义，更新到文档。</li>
<li>最好将每一个表涉及到哪一个模块进行确认，更新到文档。</li>
</ul>
<h2 id="5-环境相关注意点"><a href="#5-环境相关注意点" class="headerlink" title="5 环境相关注意点"></a>5 环境相关注意点</h2><h3 id="5-1-本地开发环境配置"><a href="#5-1-本地开发环境配置" class="headerlink" title="5.1 本地开发环境配置"></a>5.1 本地开发环境配置</h3><ul>
<li>在自己本地电脑上配置环境，将项目在自己机器上运行成功。</li>
<li>确认是否有其他的扩展，如需账号、端口，记得做记录。</li>
<li>确认是否有配置文件，如果有则对于所有的配置项进行说明。</li>
</ul>
<h3 id="5-2-测试环境"><a href="#5-2-测试环境" class="headerlink" title="5.2 测试环境"></a>5.2 测试环境</h3><p>开通测试账号。</p>
<h3 id="5-3-预上线环境"><a href="#5-3-预上线环境" class="headerlink" title="5.3 预上线环境"></a>5.3 预上线环境</h3><ul>
<li>开通预上线环境账号</li>
<li>确定预上线环境的域名地址，是否需要指定的Host等等。</li>
</ul>
<h3 id="5-4-正式环境"><a href="#5-4-正式环境" class="headerlink" title="5.4 正式环境"></a>5.4 正式环境</h3><p>开通正式环境账号</p>
<h2 id="6-对接人相关注意点"><a href="#6-对接人相关注意点" class="headerlink" title="6 对接人相关注意点"></a>6 对接人相关注意点</h2><ul>
<li>确定测试对接人。</li>
<li>确定产品对接人。</li>
<li>确定项目跨部门对接人。</li>
<li>确定运维和DBA对接人。</li>
</ul>
<h2 id="7-交接的形式"><a href="#7-交接的形式" class="headerlink" title="7 交接的形式"></a>7 交接的形式</h2><ul>
<li>具体的形式最好为举行一个交接会议，叫上相关产品、开发、测试，对于交接人员提出的问题，仔细逐一讲解、解答。</li>
<li>使用交接双方共同参与,代码讲解的方式</li>
</ul>
<h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><ul>
<li>最好能在员工提离职的时候就开始项目交接，而不是员工走的时候再做交接。</li>
<li>离职要提前与上级领导沟通，给领导留出找对接人的时间。</li>
<li>交接过程中遇到有疑问的地方，一定要确认清楚，做好记录。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/29297794" target="_blank" rel="noopener">程序员如何做好交接</a><br><a href="http://www.kuqin.com/projectmanage/20110824/263959.html" target="_blank" rel="noopener">项目交接小总结</a><br><a href="https://blog.csdn.net/shehun1/article/details/7788875" target="_blank" rel="noopener">文档交接说明书</a><br><a href="https://www.jianshu.com/p/00490a9ae109" target="_blank" rel="noopener">前端项目交接文档</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2019/02/19/20190219HandoverMemoList/">http://littleji.com/2019/02/19/20190219HandoverMemoList/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2019-02-11T16:00:00.000Z"><a href="/2019/02/12/20190212OptimizeTheDevelopmentProcess/">2019-02-12</a></time>
      
      
  
    <h1 class="title"><a href="/2019/02/12/20190212OptimizeTheDevelopmentProcess/">用gitlab来优化我们的软件开发流程</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="1-向领导汇报（内在驱动）"><a href="#1-向领导汇报（内在驱动）" class="headerlink" title="1 向领导汇报（内在驱动）"></a>1 向领导汇报（内在驱动）</h1><h2 id="员工为什么不愿意做优化"><a href="#员工为什么不愿意做优化" class="headerlink" title="员工为什么不愿意做优化"></a>员工为什么不愿意做优化</h2><ol>
<li>人本身是懒惰的，员工也是如此，他们才不会把事做好的，他们只会做相应报酬的工作量，还可能基本还达不到其相应的报酬，大多数人都在混日子啊。</li>
<li>人的天性是不喜欢改变的，人的天性是习惯于一些按部就般的事的，也许那样做令人讨厌，但是人家还是能干点东西出来。如果你逼着人家改变，你就是在压迫人家，人家自然会反抗。</li>
<li>真正了解业务的那帮人根本不可能加入项目团队，那些人谁TMD愿意和苦逼的技术人员加班啊。 那些人喜欢和我们的用户吃吃喝喝，花天酒地的，根本不会和你们那些奇怪的东西（如：backlog）或是那堆ugly的内向古怪的技术人员打交道，更别说什么技术了。</li>
<li>销售什么都干得出来，让你去做项目是因为你是廉价劳动力，而且，他们会不断地加需求，因为软件合同谈好的价格时候，连需求都没有，你去做了才有，还是模糊和不确定或根本就是错的，然后需求是越来越多，越改越多。等你精疲力尽的时候，你才意识到，销售早就把你卖了。</li>
</ol>
<h1 id="2-向开发人员讲解（外在驱动）"><a href="#2-向开发人员讲解（外在驱动）" class="headerlink" title="2 向开发人员讲解（外在驱动）"></a>2 向开发人员讲解（外在驱动）</h1><h2 id="代码审查的好处"><a href="#代码审查的好处" class="headerlink" title="代码审查的好处"></a>代码审查的好处</h2><ol>
<li>代码实现清晰易读,无法阅读自然无法审查。</li>
<li>快速学习他人的代码思路提高自身水平。</li>
<li>你确定真的开发完功能了?开发的是不是想要的功能?</li>
<li>AT &amp; T 的一个 200 多人的部门在开始执行 code review 后，开发效率提高了 14%，而错误减少了 90% 左右。</li>
</ol>
<h2 id="建立看板（scrum）开发流程的好处"><a href="#建立看板（scrum）开发流程的好处" class="headerlink" title="建立看板（scrum）开发流程的好处"></a>建立看板（scrum）开发流程的好处</h2><ol>
<li>现在让你说有三个大功能分别是（重新设计网站首页，增加用户登录功能，增加日志审计功能）问你什么时候大概能完成？帮助你评估项目的时间节点</li>
<li>一个项目上百个点，有bug、有需求，怎么开发最有效率？按照优先级来排序指定的需求</li>
<li>无休止的加需求，不干没这单，干了以后又没用，那到底干还是不干？团队审核制，不干没用的事儿<br>等等</li>
</ol>
<h2 id="建立wiki（知识管理）的好处"><a href="#建立wiki（知识管理）的好处" class="headerlink" title="建立wiki（知识管理）的好处"></a>建立wiki（知识管理）的好处</h2><ol>
<li>可以相当于是错题本，加速可以共同进步和提高</li>
</ol>
<h2 id="git的好处"><a href="#git的好处" class="headerlink" title="git的好处"></a>git的好处</h2><h2 id="持续集成的好处"><a href="#持续集成的好处" class="headerlink" title="持续集成的好处"></a>持续集成的好处</h2><ol>
<li>大型重构完，如果不跑冒烟测试、回归测试你怕不怕？自动化构建和发布项目</li>
</ol>
<h2 id="使用gitlab的好处"><a href="#使用gitlab的好处" class="headerlink" title="使用gitlab的好处"></a>使用gitlab的好处</h2><p>涵盖了上述的所有功能</p>
<h1 id="3-实施计划"><a href="#3-实施计划" class="headerlink" title="3 实施计划"></a>3 实施计划</h1><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>上来做持续集成和自动化测试显然是不现实的（假设20个人和他们的领导根本不知道自己正在开发什么的那种团队）。</p>
<ol>
<li>比较现实的是先弄清楚现在在开发哪些功能和任务，并建立一个迭代式开发的框架。否则甚至没办法弄清楚大家的工作是否可以集成。</li>
<li>但如果只做这些工作，很容易出现问题：人们渐渐地开始降低迭代交付的标准（在进度的压力下），并期待着在测试期力挽狂澜，等等。</li>
<li>这时候，比较容易的是先定一些迭代交付标准，先用这些标准来卡一下质量问题。</li>
<li>若干个迭代过后，在任何一次Release的时候，一定会出问题的！抓住这个机会，提升迭代交付标准，并采用持续集成来保证不会到Release才会出问题。</li>
<li>有了持续集成，自然会有自动化测试，因为手工集成是不可能的。</li>
<li>等持续集成和自动化测试具备后，人们已经习惯于在这个技术体系下获得Build和Release版本，任何压力已经很难让团队绕近道了。<br>当然，如果老板很早就意识到应该帮助我们而非被我们说服来做革命，我们也可以加快一点进度，在早期就引入持续集成和自动化测试。<br>但是三原则仍然是必须遵守的指导方针，换言之，即使老板是改革派，我们也别一步实现共产主义。应该以敏捷的思想逐步改变并展示回报，坚定管理者的信心，最终彻底成功。</li>
</ol>
<h2 id="阶段1"><a href="#阶段1" class="headerlink" title="阶段1"></a>阶段1</h2><ul>
<li>git工作流</li>
<li>gitlab的简单使用（人员 代码管理 常用的issue label，issue模板  ）</li>
<li>使用gitlab进行git工作流</li>
<li>gitlab上进行wiki制作从而知识分享</li>
<li>了解TDD开发</li>
<li>部署脚本</li>
</ul>
<h2 id="阶段2"><a href="#阶段2" class="headerlink" title="阶段2"></a>阶段2</h2><ul>
<li>迭代标准建立（代码覆盖率，功能点覆盖率，冒烟测试）</li>
<li>基于 gitlab 代码审核</li>
<li>基于 gitlab + jenkins持续集成、发布</li>
</ul>
<h2 id="阶段3"><a href="#阶段3" class="headerlink" title="阶段3"></a>阶段3</h2><ul>
<li>基于scrum的软件开发流程</li>
<li>基于gitlab的敏捷开发实践</li>
<li>gitlab上看板（board）的建立</li>
<li>gitlab上里程碑（milestone、小版本）建立</li>
<li>gitlab上组织划分</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://blog.csdn.net/cheny_com/article/details/5528892" target="_blank" rel="noopener">无烟会议室：CMMI vs. Scrum vs. XP（QCon 2010 感受</a><br><a href="https://waylau.com/why-we-need-continuous-integration/" target="_blank" rel="noopener">为什么我们迫切需要持续集成</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2019/02/12/20190212OptimizeTheDevelopmentProcess/">http://littleji.com/2019/02/12/20190212OptimizeTheDevelopmentProcess/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-12-24T16:00:00.000Z"><a href="/2018/12/25/20181225MLReview3/">2018-12-25</a></time>
      
      
  
    <h1 class="title"><a href="/2018/12/25/20181225MLReview3/">七天算法梳理之决策树</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h1><p>信息论的基础由香农博士于1948年奠定.下面说明关于信息论的一些基本概念.</p>
<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>上表示一个随机变量不确定的数量.如果一个随机变量的熵越大,那么其不确定也就越大.<br>如果$X$为离散型变量,取值为$\mathbb R$,其概率分布为$p(x)=P(X=x),x\in \mathbb R$,那么X的熵$H(X)$定义为:</p>
<script type="math/tex; mode=display">
H(X)=-\sum_{x \in R}p(x)log_2p(x)</script><h2 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h2><p>联合熵其实就是描述一对随机变量平均所需要的信息量.<br>如果$X,Y$是一对离散型随机变量 $X,Y ~ p(x,y),X,Y$的联合熵为$H(X,Y)$为:</p>
<script type="math/tex; mode=display">
H(X,Y)=-\sum_{x \in X}\sum_{y \in Y}p(x,y)logp(x,y)</script><h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><p>条件熵$H(Y|X)$的意思是,在X发生的条件下,Y的不确定性有</p>
<script type="math/tex; mode=display">
H(Y|X)=\sum_{x \in X}\sum_{y \in Y}p(x, y)logp(y | x)</script><p>将联合概率进行展开后发现:</p>
<script type="math/tex; mode=display">
H(X, Y)=-\sum_{x \in X}p(x)logp(x)-\sum_{x \in X}\sum_{y \in Y}p(x, y)logp(y | x) = H(X)+H(Y|X)</script><h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>现在有属性a, 其可能有v个可能的取值,如果使用属性a来对样本D进行划分的话,易知会产生v个节点,那么所有属性为$a_v$的样本可记为$D^v$.,这时候再根据各个节点对应所占的比例$|D^v|/|D|$分配权重,就可以知道使用属性a对D进行划分的时候所获得的信息增益,也就是说使用整个样本的信息熵,减去通过属性a划分的信息熵之和就是信息增益.</p>
<p>现在假设样本D的信息熵为</p>
<script type="math/tex; mode=display">
Ent(D)=-\sum_{k=1}^{|v|}p_klog_2p_k</script><p>那么信息增益为:</p>
<script type="math/tex; mode=display">
Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)</script><h2 id="基尼不纯度"><a href="#基尼不纯度" class="headerlink" title="基尼不纯度"></a>基尼不纯度</h2><p>基尼不纯度是CART算法划分属性所使用的度量方法,其直观上的理解是从一个数据集D中任意抽取两个样本,其类别不一致的概率.其具体的公式如下:</p>
<script type="math/tex; mode=display">
Gini(D)=\sum_{k=1}{|y|}\sum_{k^{'}\neq k}(p_kp_k')</script><h1 id="决策树的不同分类算法"><a href="#决策树的不同分类算法" class="headerlink" title="决策树的不同分类算法"></a>决策树的不同分类算法</h1><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><p>流程具体如下:</p>
<ol>
<li>首先考虑样本中只有一个类或者没有属性的情况</li>
<li>计算各个属性的信息增益后</li>
<li>选择信息增益最多的属性进行节点分类,建立各个节点分支</li>
<li>再依次的再各个节点中进行选择计算信息增益,返回步骤2重复迭代</li>
<li>到达指定的退出条件,没有特征或者信息增益较小<br>由于ID3 算法只有生成树的过程,没有剪枝等过程,所以可能过拟合.</li>
</ol>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>首先,信息增益比的定义是信息增益G(D,a)与训练数据集熵H(D)的比</p>
<script type="math/tex; mode=display">
g_R(D,a)=\frac{g(D,a)}{H(D)}</script><p>该C4.5算法则是针对于ID3算法的改进,在生成树的过程中使用了信息增益比来选择,而不是单纯的使用信息增益<br>算法过程如下:<br>假设 数据集D 特征集A 阀值ε</p>
<ol>
<li>如果数据中均为同一个类,则返回,算法结束</li>
<li>如果 $A=\varnothing$, 则返回一个单节点的树,并选择实例数最多的类,为该节点的类别,算法结束</li>
<li>选择其中信息增益比最大的节点</li>
<li>再依次选择各个节点,计算当前节点的内的信息增益比,进行迭代</li>
<li>最终达到指定的退出条件,即信息增益比过低,或者没有更多的特征时退出算法</li>
</ol>
<p>上面的构建的节点树都是分类树,只不过节点划分的方式不同.那么什么是回归树呢?</p>
<h1 id="回归树原理"><a href="#回归树原理" class="headerlink" title="回归树原理"></a>回归树原理</h1><p>回归树对于样本的划分,通过遍历所有输入变量，找到最优的切分变量j和最优的切分点s，即选择第j个特征$x^j$和它的取值s将输入空间划分为两部分，然后重复这个操作,对于连续性的样本值非常有效.<br>具体算法如下</p>
<ol>
<li>选择最优的切分变量j和最优的切分点s，求解 <script type="math/tex; mode=display">
min_{j,s}[min_{c_{1}}\sum_{x_{i}\in R_{1}(j,s)}(y_{i}-c_{1})^2+min_{c_{2}}\sum_{x_{i}\in R_{2}(j,s)}(y_{i}-c_{2})^2]</script></li>
<li>遍历所有特征，对固定的特征扫描所有取值，找到使上式达到最小值的对(j,s).</li>
<li>用选定的对 (j,s)划分区域，并确定该区域的预测值；</li>
<li>继续对两个字区域调用上述步骤，直至满足停止条件；</li>
</ol>
<h2 id="CART分类树"><a href="#CART分类树" class="headerlink" title="CART分类树"></a>CART分类树</h2><p>CART分类树的全称是分类与回归树,主要的原理思想是将内部的节点特征取值为”是”或”否”两个值,左分支为是,右分支为否,这样整个决策树就可以在整个样本空间中求取对应的条件概率分布.<br>算法由特征选择和生成树以及前面两种算法所没有的剪枝构成,算法主要包括两个部分:树的生成与剪枝</p>
<h3 id="CART的生成"><a href="#CART的生成" class="headerlink" title="CART的生成"></a>CART的生成</h3><p>从根节点开始，对节点计算现有特征的基尼指数，对每一个特征，例如AA，再对其每个可能的取值如aa,根据样本点对A=aA=a的结果的”是“与”否“划分为两个部分，利用</p>
<script type="math/tex; mode=display">
Gini(D,A=a)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})</script><p>进行计算；在所有可能的特征AA以及该特征所有的可能取值a中，选择基尼指数最小的特征及其对应的取值作为最优特征和最优切分点。然后根据最优特征和最优切分点，将本节点的数据集二分，生成两个子节点<br>对两个字节点递归地调用上述步骤，直至节点中的样本个数小于阈值，或者样本集的基尼指数小于阈值，或者没有更多特征后停止；</p>
<h3 id="CART的剪枝"><a href="#CART的剪枝" class="headerlink" title="CART的剪枝"></a>CART的剪枝</h3><p>剪枝就是对生成的树进行裁剪简化的过程,其一般是通过极小化决策树整体的损失函数或代价函数来实现.<br>CART的剪枝是通过两个步骤:</p>
<ol>
<li>从树的底部不断地剪枝直到根节点,形成对应的子树序列</li>
<li>通过交叉验证法,对子树的序列进行测试,并从中选取最优的子树</li>
</ol>
<h1 id="决策树防止过拟合手段"><a href="#决策树防止过拟合手段" class="headerlink" title="决策树防止过拟合手段"></a>决策树防止过拟合手段</h1><p>决策树过拟合主要有两个手段,分别为early stopping与剪枝.</p>
<ol>
<li>earlystopping:限制选取的分类节点的总数,树的深度,节点中的实例数,阈值等</li>
<li>剪枝,即当前节点的划分无法带来决策树泛化性能的提升,增删除对应的节点</li>
</ol>
<h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><p>可以使用之前梳理的AUC ROC 交叉验证 随机抽样等方法,这里就不再赘述了.</p>
<h1 id="python可视化决策树与对应的函数实现"><a href="#python可视化决策树与对应的函数实现" class="headerlink" title="python可视化决策树与对应的函数实现"></a>python可视化决策树与对应的函数实现</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import pydotplus</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn import tree</span><br><span class="line">import collections</span><br><span class="line"># Data Collection</span><br><span class="line">X = [ [180, 15,0],     </span><br><span class="line">      [177, 42,0],</span><br><span class="line">      [136, 35,1],</span><br><span class="line">      [174, 65,0],</span><br><span class="line">      [141, 28,1]]</span><br><span class="line"></span><br><span class="line">Y = [&apos;man&apos;, &apos;woman&apos;, &apos;woman&apos;, &apos;man&apos;, &apos;woman&apos;]    </span><br><span class="line"></span><br><span class="line">data_feature_names = [ &apos;height&apos;, &apos;hair length&apos;, &apos;voice pitch&apos; ]</span><br><span class="line"># Training</span><br><span class="line">clf = tree.DecisionTreeClassifier()</span><br><span class="line">clf = clf.fit(X,Y)</span><br><span class="line"># Visualize data</span><br><span class="line">dot_data = tree.export_graphviz(clf,</span><br><span class="line">                                feature_names=data_feature_names,</span><br><span class="line">                                out_file=None,</span><br><span class="line">                                filled=True,</span><br><span class="line">                                rounded=True)</span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data)</span><br><span class="line"></span><br><span class="line">colors = (&apos;turquoise&apos;, &apos;orange&apos;)</span><br><span class="line">edges = collections.defaultdict(list)</span><br><span class="line"></span><br><span class="line">for edge in graph.get_edge_list():</span><br><span class="line">    edges[edge.get_source()].append(int(edge.get_destination()))</span><br><span class="line"></span><br><span class="line">for edge in edges:</span><br><span class="line">    edges[edge].sort()    </span><br><span class="line">    for i in range(2):</span><br><span class="line">        dest = graph.get_node(str(edges[edge][i]))[0]</span><br><span class="line">        dest.set_fillcolor(colors[i])</span><br><span class="line"></span><br><span class="line">graph.write_png(&apos;tree.png&apos;)</span><br></pre></td></tr></table></figure>
<p>主要的函数为</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="">统计自然语言处理-宗成庆</a><br><a href="">机器学习-周志华</a><br><a href="">统计学习方法-李航</a><br><a href="https://blog.csdn.net/weixin_36586536/article/details/80468426" target="_blank" rel="noopener">决策树(分类树、回归树</a><br><a href="https://pythonprogramminglanguage.com/decision-tree-visual-example/" target="_blank" rel="noopener">Decision tree visual example</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2018/12/25/20181225MLReview3/">http://littleji.com/2018/12/25/20181225MLReview3/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-12-21T16:00:00.000Z"><a href="/2018/12/22/20181222MLReview2/">2018-12-22</a></time>
      
      
  
    <h1 class="title"><a href="/2018/12/22/20181222MLReview2/">七天算法梳理之逻辑回归</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="逻辑回归与线性回归的联系与区别"><a href="#逻辑回归与线性回归的联系与区别" class="headerlink" title="逻辑回归与线性回归的联系与区别"></a>逻辑回归与线性回归的联系与区别</h1><p>逻辑回归事实上是将线性回归的输出进行了非线性函数的映射,而这个映射即是:</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-(w^{T}x+b)}}</script><h1 id="逻辑回归的原理"><a href="#逻辑回归的原理" class="headerlink" title="逻辑回归的原理"></a>逻辑回归的原理</h1><p>逻辑回归的主要原理是将之前的线性空间通过非线性函数进行再输出,让对应的输出范围集中在要么靠近0,要么靠近1的区域内,从而完成将对应的数据分类的目的</p>
<p>3、逻辑回归损失函数推导及优化<br>假设<br>P(y=1|x,θ)=hθ(x)<br>P(y=0|x,θ)=1−hθ(x)<br>则有<br>P(y|x,θ)=hθ(x)y(1−hθ(x))1−y<br>很容易得到似然函数表达式:<br>L(θ)=∏i=1m(hθ(x(i)))y(i)(1−hθ(x(i)))1−y(i)<br>取对数得:<br>J(θ)=−lnL(θ)=−∑i=1m(y(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i))))</p>
<p>4、 正则化与模型评估指标<br>逻辑回归也需要处理过拟合的问题那么正则化的方法提供了一个很好地思路<br>逻辑回归的L1正则化的损失函数表达式如下，相比普通的逻辑回归损失函数，增加了L1的范数做作为惩罚，超参数α作为惩罚系数，调节惩罚项的大小。</p>
<p>二元逻辑回归的L1正则化损失函数表达式如下：<br>J(θ)=−Y⊙loghθ(X)−(E−Y)⊙log(E−hθ(X))+||θ||1<br>其中||θ||1为θ的L1范数。</p>
<p>二元逻辑回归的L2正则化损失函数表达式如下：<br>J(θ)=−Y⊙loghθ(X)−(E−Y)⊙log(E−hθ(X))+12α||θ||22<br>其中||θ||2为θ的L2范数。</p>
<p>5、逻辑回归的优缺点<br>优点:可以给出概率,解释性较好<br>缺点:容易欠拟合,对于非线性的特征还需要进一步的转化,</p>
<p>6、样本不均衡问题解决办法<br>类别不平衡问题指的是当正反例的数目偏差过大的时候，所造成的困扰<br>类别不平衡的一个基本策略是-再缩放(rescaling)<br>主要有三个途径:</p>
<ol>
<li>对训练集中的反类样例进行欠采样,去除一些反例</li>
<li>对训练集里的正例进行过采样</li>
<li><p>直接学习但在预测的时候进行阀值的改变<br>类别不平衡学习通常是较小类的代价更高,</p>
</li>
<li><p>sklearn参数<br>逻辑回归具体的位置在:<br>from sklearn.linear_model import LogisticRegression<br>主要有C penalty tol solver 等几个参数<br>C:正则化系数的倒数,默认为1<br>penalty:用来指定正则化的参数<br>tol:迭代终止的误差范围<br>solver:决定使用什么样的优化方法</p>
</li>
</ol>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2018/12/22/20181222MLReview2/">http://littleji.com/2018/12/22/20181222MLReview2/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-12-17T16:00:00.000Z"><a href="/2018/12/18/20181218MLReview1/">2018-12-18</a></time>
      
      
  
    <h1 class="title"><a href="/2018/12/18/20181218MLReview1/">七天算法梳理之线性回归</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="1-机器学习的一些概念"><a href="#1-机器学习的一些概念" class="headerlink" title="1 机器学习的一些概念"></a>1 机器学习的一些概念</h1><h2 id="1-1-什么是机器学习"><a href="#1-1-什么是机器学习" class="headerlink" title="1.1 什么是机器学习?"></a>1.1 什么是机器学习?</h2><p>计算机系统能够通过现有的数据，不断地改进其解决某一问题的性能，称为机器学习。</p>
<h2 id="1-2-什么是有监督与无监督学习？"><a href="#1-2-什么是有监督与无监督学习？" class="headerlink" title="1.2 什么是有监督与无监督学习？"></a>1.2 什么是有监督与无监督学习？</h2><p>对于机器学习的过程中，对于所要学习的数据，如果其为标注的数据则称为有监督学习，否则称为无监督学习。</p>
<h2 id="1-3-什么是泛化能力？"><a href="#1-3-什么是泛化能力？" class="headerlink" title="1.3 什么是泛化能力？"></a>1.3 什么是泛化能力？</h2><p>泛化能力指一个经过学习后的模型，能够适应与处理新样本的能力，这也是机器学习的主要目标。</p>
<h2 id="1-4-什么是过拟合与欠拟合？"><a href="#1-4-什么是过拟合与欠拟合？" class="headerlink" title="1.4 什么是过拟合与欠拟合？"></a>1.4 什么是过拟合与欠拟合？</h2><ul>
<li>过拟合：过拟合指在学习的过程中，模型把样本本身的特点而非目标模型的特点进行了学习，导致整体性能下降的情况。</li>
<li>欠拟合：欠拟合则是对目标模型的一般性质还没有习得，主要是由于模型本身的学习与表达能力不够。</li>
</ul>
<h2 id="1-5-过拟合与欠拟合如何解决？"><a href="#1-5-过拟合与欠拟合如何解决？" class="headerlink" title="1.5 过拟合与欠拟合如何解决？"></a>1.5 过拟合与欠拟合如何解决？</h2><ul>
<li>欠拟合的解决：通过增加模型的学习能力，如：增加学习轮数、增加神经网络的复杂度以提升其表现能力等。</li>
<li>过拟合的解决：过拟合一般来说是只可缓解无法彻底解决，通过对学习使用的模型进行改进，如 ：减少相应的模型参数、降低神经网络复杂度等。</li>
</ul>
<h2 id="1-6-什么是方差Variance和偏差Bias？"><a href="#1-6-什么是方差Variance和偏差Bias？" class="headerlink" title="1.6 什么是方差Variance和偏差Bias？"></a>1.6 什么是方差Variance和偏差Bias？</h2><p>方差与偏差是解释学习算法泛化性能的一种重要的工具，下面我们来说明二者的侧重点。<br>首先,我们将所得到的训练数据等分成A,B,C3份,再借由这3份训练数据分别得出3个训练模型,那么”方差”指的是向这3个模型输入同样的测试样本,得出的输出结果的方差,借此来判断我们选择的模型对于不同的学习样本展现出的”模型本身训练的稳定性”.</p>
<p>其次,如果我们将上面的所有训练数据进行训练,并输入同样的测试样本,得到对应的输出结果X,将输出结果与真实的结果T进行比较,得到的差值就为偏差, 偏差则侧重于”模型本身预测的精准度”的衡量.</p>
<p>方差,偏差,噪声三者共同主导了学习模型的误差,在学习初期由于模型的拟合能力不强,这个时候主要是由偏差主导了误差,当学习后期模型的拟合能力增强后,微小的数据扰动都会被模型所捕捉到,此时由方差来主导模型的误差.</p>
<h2 id="1-7-交叉验证"><a href="#1-7-交叉验证" class="headerlink" title="1.7 交叉验证"></a>1.7 交叉验证</h2><p>为了能够具体的衡量模型的性能,交叉验证提供了这样一种性能评测的手段.</p>
<p>交叉验证的思想是重复的利用数据,把数据进行切分为训练数据集与测试数据集,并在这个基础上进行反复的训练与测试,选取具有较好性能指标的模型.</p>
<p>具体的思想是,在数据集上划分k个大小相同且互斥的子集,使用k-1个子集进行训练,最后一个进行测试,得出结果后,再选择不同的k-1个自己训练,最后一个进行测试,容易看出上面可最终得到k个模型,通过求取其平均误差得到该模型的测试误差.</p>
<p>其中典型的k值为10.</p>
<h1 id="2-线性模型"><a href="#2-线性模型" class="headerlink" title="2 线性模型"></a>2 线性模型</h1><h2 id="2-1-线性回归的原理"><a href="#2-1-线性回归的原理" class="headerlink" title="2.1 线性回归的原理"></a>2.1 线性回归的原理</h2><ul>
<li>线性模型:即通过将给定的一些特征进行线性组合所得到的模型.</li>
<li>线性回归:通过学习得到一个线性的模型能够尽可能准确的预测输入数据的真实标记.</li>
<li>线性回归的原理:使用了均方误差最小化的方法也就是常说的最小二乘法,即试图找到这么一条直线,使样本到直线上的欧氏距离之和最小.</li>
</ul>
<h2 id="2-2-线性回归损失函数、代价函数、目标函数"><a href="#2-2-线性回归损失函数、代价函数、目标函数" class="headerlink" title="2.2 线性回归损失函数、代价函数、目标函数"></a>2.2 线性回归损失函数、代价函数、目标函数</h2><ol>
<li>损失函数指的是单一训练集上产生的误差.</li>
<li>代价函数则值得是模型在整个训练集上产生的平均误差.</li>
<li><p>设学习后的模型f,面对测试样本X,模型对应的模拟输出f(X)以及X实际的输出Y,拟合的程度.有如下的表示方式</p>
<script type="math/tex; mode=display">
L(Y, f(X))=(Y-f(X))^2</script></li>
<li><p>但是仅仅通过损失函数来就纠正拟合误差并非我们的目标,我们的是目标是是让模型精确地同时,又尽量的减少模型的复杂度,于是就引入了正则化函数来衡量模型的复杂度,最终我们的目标函数是最小化误差与最小化模型复杂度之和也就是</p>
<script type="math/tex; mode=display">
\frac{1}{N} \sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)</script><p>前者是最小化经验风险,后者则是最小化结构风险</p>
</li>
</ol>
<h1 id="3-线性模型的优化方法"><a href="#3-线性模型的优化方法" class="headerlink" title="3 线性模型的优化方法"></a>3 线性模型的优化方法</h1><p>当我们得到了对应目标函数后,那么就需要具体的对该模型各个参数求最优解,也就是常见的最优化问题,这里有以下这么几个主要的算法.</p>
<h2 id="3-1-梯度下降法"><a href="#3-1-梯度下降法" class="headerlink" title="3.1 梯度下降法"></a>3.1 梯度下降法</h2><p>梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向.</p>
<h2 id="3-2-随机梯度下降法"><a href="#3-2-随机梯度下降法" class="headerlink" title="3.2 随机梯度下降法"></a>3.2 随机梯度下降法</h2><p>由于梯度下降法使用了固定步长的,这样带来了后期收敛慢,其进入极小点的情况,这里通过选用随机梯度的下降,更容易突破局部极小点,从而收敛至全局极值点,不过也有迭代次数增加等缺点.</p>
<h2 id="3-3-牛顿法"><a href="#3-3-牛顿法" class="headerlink" title="3.3 牛顿法"></a>3.3 牛顿法</h2><p>牛顿法不同于梯度向量,而是使用了二阶海森矩阵的逆矩阵来求解,相对来说比普通的梯度下降算法收敛速度更快,但是其要求必须具有二阶海森矩阵的逆矩阵条件,其计算非常复杂,该条件在大规模数据下往往无法保证.</p>
<h2 id="3-4-拟牛顿法"><a href="#3-4-拟牛顿法" class="headerlink" title="3.4 拟牛顿法"></a>3.4 拟牛顿法</h2><p>拟牛顿法则是通过找到一个与海森矩阵类似性质的矩阵来替代,从而让计算更为容易.<br>其中DFP BFGS L-BFGS都是比较重要的拟牛顿方法.</p>
<h2 id="3-5-其他自适应学习方法"><a href="#3-5-其他自适应学习方法" class="headerlink" title="3.5 其他自适应学习方法"></a>3.5 其他自适应学习方法</h2><p>adagrad,adadelta,rmsprop,adam等一系列adaptive learning rate方法</p>
<h1 id="4-线性回归的评估指标"><a href="#4-线性回归的评估指标" class="headerlink" title="4 线性回归的评估指标"></a>4 线性回归的评估指标</h1><h2 id="4-1查准率-全查率与F1"><a href="#4-1查准率-全查率与F1" class="headerlink" title="4.1查准率 全查率与F1"></a>4.1查准率 全查率与F1</h2><p>二分类问题有四种预测的情况:<br>真正例:true positive<br>假正例:false positive<br>真反例:true negative<br>假反例:false negative<br>查准率P与全查率R为</p>
<script type="math/tex; mode=display">
P=\frac{TP}{TP+FP}</script><script type="math/tex; mode=display">
 R=\frac{TP}{TP+FN}</script><p> 二者一般矛盾,因为查全率高意味着查的个数多,这样又会导致查准率下降<br>由查全率作为横轴,查准率作为纵轴形成P-R曲线,比较学习期的的性能时,比较该曲线的面积是一个方法<br>平衡点(Break-Event Point)是查准率等于查全率的取值,这样可以权衡查全率和查准率<br>BEP还是有些简化,更常用的是F1度量,目的是为了找出更好的学习器<br>F1:基于查准率和查全率的调和平均值(harmonic mean):</p>
<script type="math/tex; mode=display">
F1=\frac{2*P*R}{P+R}=\frac{2*TP}{样例总数+TP-TN}</script><script type="math/tex; mode=display">
F_\beta=\frac{(1+\beta^{2})*P*R}{\beta^2*P +R}</script><p> 其中β&gt;0, β=1时就退化为标准的F1,β<1时则查准更重要,β>1则查全更重要<br>很多时候我们有多个二分类混淆矩阵,例如进行多次训练和测试,每次都有一个,或者执行多分类任务,每每两个组合都对应一个混淆矩阵<br>一种直接的做法是在所有的混淆矩阵都计算,然后计算所有的平均值也就是宏查重率macro-P和macro-R以及对应的macro F1</1时则查准更重要,β></p>
<h2 id="4-2-ROC与AUC"><a href="#4-2-ROC与AUC" class="headerlink" title="4.2 ROC与AUC"></a>4.2 ROC与AUC</h2><p>学习预测就是将样本进行排序,最可能是正例的排在前面<br>神经网络一般情形下对每个测试样本预测出一个0-1的实值,然后将这个值作为截断点,大于这个截断点的样本为正例,其他的为反例,如果更重视查准率则可选择排序中靠前的位置,重视查全率则选择较后的截断点<br>排序本身的质量好环则是体现了学习器在不同任务下的期望泛化性能,ROC曲线就是从这个角度来研究学习器泛化性能</p>
<p>ROC(Receiver Operating Characteristic)的操作方式与之前的P-R图类似,并提出了两个概念,真正利率TPR,假正利率FPR</p>
<script type="math/tex; mode=display">
TPR=TP/(TP+FN)</script><script type="math/tex; mode=display">
FPR=FP/(FP+TN)</script><h2 id="4-3-ROC绘制过程"><a href="#4-3-ROC绘制过程" class="headerlink" title="4.3 ROC绘制过程"></a>4.3 ROC绘制过程</h2><p>设正例数目为m+ 反例数目为m-</p>
<ol>
<li>均是先对所有的样例根据学习器的结果进行排序,然后将分类的阀值设置为最大,这时候均是反例,TPR=FPR=0<br>2.调整阀值为依次每个样例的值,然后观察次样本是否为真正例,如果是坐标为(x,y+1/m+),如果不是坐标为(x+1/m-,y)</li>
<li>比较ROC曲线的面积也就是AUC</li>
</ol>
<p>AUC更考虑的是样本预测的排序质量</p>
<h1 id="5-sklearn参数详解"><a href="#5-sklearn参数详解" class="headerlink" title="5 sklearn参数详解"></a>5 sklearn参数详解</h1><p>下面对于一个最普通的sklearn 线性模型的使用方式进行说明<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个普通的线性模型</span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"># 输入对应的训练数据x,以及对应的标签数据y</span><br><span class="line">regr.fit(datasets_train_x, datasets_train_y)</span><br><span class="line"></span><br><span class="line"># 输入对对应的测试数据x,得出模型的预测输出</span><br><span class="line">pred_y = regr.predict(datasets_test_x)</span><br><span class="line"></span><br><span class="line"># 最终使用下面的命令轲输出对应的w 与 b</span><br><span class="line">reg.coef_</span><br></pre></td></tr></table></figure></p>
<p>除了上面的模型还有包括 lasso ridge等回归模型在<code>linear_model</code>包内</p>
<h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><h2 id="如何提升泛化能力"><a href="#如何提升泛化能力" class="headerlink" title="如何提升泛化能力?"></a>如何提升泛化能力?</h2><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://scikit-learn.org/stable/modules/linear_model.html" target="_blank" rel="noopener">scikit-learn 0.20 document</a><br><a href="https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/linear-regression" target="_blank" rel="noopener">azure machine-learning-reference</a><br><a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">deeplearning</a><br><a href="https://www.amazon.cn/dp/B01ARKEV1G/ref=sr_1_1?ie=UTF8&amp;qid=1545063695&amp;sr=8-1&amp;keywords=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0+%E5%91%A8%E5%BF%97%E5%8D%8E" target="_blank" rel="noopener">机器学习-周志华</a><br><a href="https://www.amazon.cn/dp/B007TSFMTA/ref=pd_sbs_14_3?_encoding=UTF8&amp;pd_rd_i=B007TSFMTA&amp;pd_rd_r=d8a70aa5-0217-11e9-a072-e9dfeb7db3a3&amp;pd_rd_w=PIcAR&amp;pd_rd_wg=9xBdB&amp;pf_rd_p=2d9f2e80-85a0-476d-89d7-9e61ddceb885&amp;pf_rd_r=WPZZ4JN9QDGJN49CCH7G&amp;psc=1&amp;refRID=WPZZ4JN9QDGJN49CCH7G" target="_blank" rel="noopener">统计学习方法-李航</a><br><a href="(https://www.zhihu.com/question/27068705">机器学习中的Bias(偏差,Error(误差)和Variance(方差))有什么区别与联系</a><br><a href="http://oath2yangmen.online/2018/01/29/%E7%90%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">理解机器学习中常用优化方法</a><br><a href="https://www.zhihu.com/question/52398145/answer/209358209" target="_blank" rel="noopener">机器学习中的目标函数\损失函数\代价函数</a><br><a href="https://www.zhihu.com/question/46441403" target="_blank" rel="noopener">梯度下降or拟牛顿法?</a><br><a href="https://zhuanlan.zhihu.com/p/37524275" target="_blank" rel="noopener">梯度下降法\牛顿法和拟牛顿法</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2018/12/18/20181218MLReview1/">http://littleji.com/2018/12/18/20181218MLReview1/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-11-26T16:00:00.000Z"><a href="/2018/11/27/20181127GenerativeVSDiscriminative/">2018-11-27</a></time>
      
      
  
    <h1 class="title"><a href="/2018/11/27/20181127GenerativeVSDiscriminative/">Gnerative VS Discriminative</a></h1>
  

    </header>
    <div class="entry">
      
        <blockquote>
<p>“one should solve<br>the [classification] problem directly and never solve a more general problem as an<br>intermediate step [such as modeling p(xly)].”  —-Vapnik </p>
</blockquote>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>笔者在 NLP 概率图学习的过程中,发现解决同一种问题可以使用若干种模型,而大多数的 NLP 模型的种类主要集中在有监督学习.<br>通过概率图,我们自然会联想到对应的概率问题,而每当概率问题出现,无可避免的会有频率学派与贝叶斯派的竞争,两派为了解决同一个问题开发出了不同的概率图模型,自然在这些概率图的模型分类中也出现了对应分支,分别为判别式与生成式.<br>两个模型本身是解释同一种问题的不同角度,笔者最近对于该两个模型之间的对比与解释进行了学习,整理如下:</p>
<h1 id="二个模型在分类问题上的处理方式"><a href="#二个模型在分类问题上的处理方式" class="headerlink" title="二个模型在分类问题上的处理方式"></a>二个模型在分类问题上的处理方式</h1><p>判别模型不关心数据是如何生成的，它只是对给定数据进行分类。<br>因此，判别算法试图直接从数据中学习P（y | x），然后尝试对数据进行分类。<br>另一方面，生成模型试图学习p（x，y），后来根据条件概率公式，可以将其转换为p（y | x）来对数据进行分类。</p>
<h1 id="为了便于理解这里举两个例子"><a href="#为了便于理解这里举两个例子" class="headerlink" title="为了便于理解这里举两个例子"></a>为了便于理解这里举两个例子</h1><h2 id="第一个例子"><a href="#第一个例子" class="headerlink" title="第一个例子"></a>第一个例子</h2><p>当我们需要判断两种不同的语言(比如中文和英文)的时候:</p>
<ol>
<li>生成式模型:先去学习这两种不同的语言,再根据语言的输入去判断语言的种类</li>
<li>判断式模型:直接根据输入判断到底属于哪种语言</li>
</ol>
<h2 id="第二个例子"><a href="#第二个例子" class="headerlink" title="第二个例子"></a>第二个例子</h2><p>假设有<br><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82eadd7786ea06b1d32108962c79118245872703" alt=""><br>以及<br><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7c66170a51fd7b916d42c2cf8e8512c75c85a594" alt=""><br>以及对应的几个数据:<br><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b9386c4024115b82d7cb8135d3a1a589ec87e16" alt=""><br>那么根据定义就可以求出联合概率分布为</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>p(x,y)</th>
<th>y=0</th>
<th>y=1</th>
</tr>
</thead>
<tbody>
<tr>
<td> x=1</td>
<td>1/2</td>
<td>0</td>
</tr>
<tr>
<td> x=2</td>
<td>1/4</td>
<td>1/4</td>
</tr>
</tbody>
</table>
</div>
<p>对应的条件分布概率则为</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>p(y\</th>
<th>x)</th>
<th>y=0</th>
<th>y=1</th>
</tr>
</thead>
<tbody>
<tr>
<td> x=1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td> x=2</td>
<td>1/2</td>
<td>1/2</td>
</tr>
</tbody>
</table>
</div>
<h1 id="一张图理解生成模型与判别模型的关系"><a href="#一张图理解生成模型与判别模型的关系" class="headerlink" title="一张图理解生成模型与判别模型的关系"></a>一张图理解生成模型与判别模型的关系</h1><p><img src="https://datawarrior.files.wordpress.com/2016/05/discriminative_vs_generative.png?w=1314" alt=""></p>
<p>学习的目标是正确的将未知的数据进行分类,从图中我们可以很容易的看出:</p>
<ol>
<li>判别模型学习得到的是那条分类的曲线,其关注点在于分类的边界学习</li>
<li>生成模型则学习得到的则是两类数据的具体分布情况</li>
</ol>
<h1 id="生成模型与判别模型的优缺点"><a href="#生成模型与判别模型的优缺点" class="headerlink" title="生成模型与判别模型的优缺点"></a>生成模型与判别模型的优缺点</h1><h2 id="生成模型的优点"><a href="#生成模型的优点" class="headerlink" title="生成模型的优点"></a>生成模型的优点</h2><ol>
<li>在不平衡的数据样本上,表现依然优异</li>
<li>可输出所有类别下的估算概率</li>
<li>更好的模型解释性</li>
<li>更像是通用型 AI,可以产生有语法错误答案 有口音的语音等,可以使用 p（x，y）生成类似于现有数据的新数据</li>
<li>当样本数量较多时，生成模型能更快地收敛于真实模型</li>
<li>生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法</li>
<li>只有生成模型能检测异常值。由于生成模型完全学习了所有的分布，所以它可以用来检测某个值是否异常：P(X)是否太小</li>
</ol>
<h2 id="生成模型的缺点"><a href="#生成模型的缺点" class="headerlink" title="生成模型的缺点"></a>生成模型的缺点</h2><ol>
<li>联合分布是能提供更多的信息，但也需要更多的样本和更多计算，尤其是为了更准确估计类别条件分布，需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到，因而如果我们只需要做分类任务，就浪费了计算资源</li>
</ol>
<h2 id="判别模型的优点"><a href="#判别模型的优点" class="headerlink" title="判别模型的优点"></a>判别模型的优点</h2><ol>
<li>在拥有大量的数据集的时候,相对于生成式模型,其准确度更高</li>
<li>由于直接学习P(\tilde{c}|\tilde{x} )，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题</li>
<li>相对于生成模型来说,其计算资源大大地节省了,性能较好</li>
<li>所需要的样本数量少于生成模型</li>
</ol>
<h2 id="判别模型的缺点"><a href="#判别模型的缺点" class="headerlink" title="判别模型的缺点"></a>判别模型的缺点</h2><ol>
<li>不适合应用在不平衡的数据集中</li>
<li>只能应用在监督学习的任务中</li>
<li>模型的解释性差</li>
<li>尽管判别模型不需要对观察到的变量的分布进行建模，但它们通常不能表达观察变量和目标变量之间的复杂关系。在分类和回归任务中，它们不一定比生成模型表现更好。</li>
</ol>
<h1 id="主要的生成模型"><a href="#主要的生成模型" class="headerlink" title="主要的生成模型"></a>主要的生成模型</h1><ol>
<li>LDA</li>
<li>HMM</li>
<li>朴素贝叶斯</li>
<li>混合高斯模型</li>
<li>概率上下无关文法</li>
<li>变分自动编码器</li>
<li>GAN</li>
</ol>
<h1 id="主要的判别模型"><a href="#主要的判别模型" class="headerlink" title="主要的判别模型"></a>主要的判别模型</h1><ol>
<li>LR</li>
<li>SVM</li>
<li>CRF</li>
<li>Boosting</li>
<li>Decision tree</li>
<li>K-neighbor</li>
<li>最大熵模型</li>
<li>感知机</li>
<li>神经网络</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.zhihu.com/question/20446337" target="_blank" rel="noopener">知乎:机器学习“判定模型”和“生成模型”有什么区别？</a><br><a href="https://en.wikipedia.org/wiki/Generative_model" target="_blank" rel="noopener">Generative model</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2018/11/27/20181127GenerativeVSDiscriminative/">http://littleji.com/2018/11/27/20181127GenerativeVSDiscriminative/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-10-24T16:00:00.000Z"><a href="/2018/10/25/20181025LogstashGrokOverview/">2018-10-25</a></time>
      
      
  
    <h1 class="title"><a href="/2018/10/25/20181025LogstashGrokOverview/">Logstash Grok 概览</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>Grok 插件让 Logstash 在处理一定格式的日志文件的时候有了一定程度“正则表达”的功能。<br>特别适用于解析一些非结构化的数据，并从中提取真正相关的信息至一些结构化 field 中。</p>
<p>主要的使用场景在 syslog、apache 与其他的 webserver 具有这样特点的日志：这些日志都具有易人读但不易机读的特点。</p>
<p>Logstash 默认有很多 patterns，你可以在 <a href="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns" target="_blank" rel="noopener">https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns</a> 找到。当然你也可以添加你自己的 pattern，这点我们稍后就会讲到。</p>
<h1 id="测试你的-Grok-pattern"><a href="#测试你的-Grok-pattern" class="headerlink" title="测试你的 Grok pattern"></a>测试你的 Grok pattern</h1><p>Debug 是人类进步的阶梯，想起来当时被正则表达式支配的恐惧了么？那时我们有一些工具来测试我们的正则表达式是否足够“强壮”，grok 的作者也提供了两个网站，允许你通过这个来测试你的 grok 表达式，就跟你在测试正表达式的时候一样。分别是 <a href="http://grokdebug.herokuapp.com" target="_blank" rel="noopener">http://grokdebug.herokuapp.com </a> 和 <a href="http://grokconstructor.appspot.com/" target="_blank" rel="noopener"> http://grokconstructor.appspot.com/</a></p>
<h1 id="Grok-的一些基本设定"><a href="#Grok-的一些基本设定" class="headerlink" title="Grok 的一些基本设定"></a>Grok 的一些基本设定</h1><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>Grok 使用如下的语法进行匹配操作。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%&#123;SYNTAX:SEMANTIC&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中 <code>SYNTAX</code>是你想匹配的表达式的名称，SEMANTIC 则是存储匹配后的文本的变量名，比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 待匹配的文本为</span><br><span class="line">192.168.1.1 2016-04-26T19:55:15Z</span><br><span class="line"># 一个完整的匹配式</span><br><span class="line">%&#123;IPV4:ip&#125; %&#123;TIMESTAMP_ISO8601:time&#125;</span><br><span class="line"># pattern 为</span><br><span class="line">IPV4 (?&lt;![0-9])(?:(?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]&#123;1,2&#125;)[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]&#123;1,2&#125;)[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]&#123;1,2&#125;)[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]&#123;1,2&#125;))(?![0-9])</span><br><span class="line"></span><br><span class="line">TIMESTAMP_ISO8601 %&#123;YEAR&#125;-%&#123;MONTHNUM&#125;-%&#123;MONTHDAY&#125;[T ]%&#123;HOUR&#125;:?%&#123;MINUTE&#125;(?::?%&#123;SECOND&#125;)?%&#123;ISO8601_TIMEZONE&#125;?</span><br></pre></td></tr></table></figure></p>
<p>那么 <code>IPV4</code> 和 <code>TIMESTAMP_ISO8601</code>就是 <code>SYNTAX</code>，<code>ip</code> 和 <code>time</code> 就是 SEMANTIC。</p>
<h2 id="一个实际的例子"><a href="#一个实际的例子" class="headerlink" title="一个实际的例子"></a>一个实际的例子</h2><p>一个 logstash 的配置文件，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">  file &#123;</span><br><span class="line">    path =&gt; &quot;/var/log/http.log&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IP:client&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;&quot; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行后，这条 message 将会在 grok 过滤之后上报一条 event，内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">client: 55.3.244.1</span><br><span class="line">method: GET</span><br><span class="line">request: /index.html</span><br><span class="line">bytes: 15824</span><br><span class="line">duration: 0.043</span><br></pre></td></tr></table></figure></p>
<h1 id="正则表达式库"><a href="#正则表达式库" class="headerlink" title="正则表达式库"></a>正则表达式库</h1><p>Grok 使用了 <a href="https://github.com/kkos/oniguruma/blob/master/doc/RE" target="_blank" rel="noopener">oniguruma</a> 的正则表达式，所以所有在该文档中出现的匹配语法都可以使用。</p>
<h1 id="自定义-pattern"><a href="#自定义-pattern" class="headerlink" title="自定义 pattern"></a>自定义 pattern</h1><p>Grok 自带了一些常用的匹配规则，比如：IPV4、IPV6、YEAR等，但是当我们想要写一些自己的规则的时候，grok 也提供了支持，这里主要有两种方式：</p>
<h2 id="第一种方式添加自定义-pattern"><a href="#第一种方式添加自定义-pattern" class="headerlink" title="第一种方式添加自定义 pattern"></a>第一种方式添加自定义 pattern</h2><p>直接使用下面的 pattern 结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(?&lt;field_name&gt;the pattern here)</span><br></pre></td></tr></table></figure></p>
<p>其中 field_name 可以理解为上文的 SEMANTIC，后面则跟的是实际的匹配表达式，比如这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(?&lt;queue_id&gt;[0-9A-F]&#123;10,11&#125;)</span><br></pre></td></tr></table></figure></p>
<h2 id="第二种方式添加自定义-pattern"><a href="#第二种方式添加自定义-pattern" class="headerlink" title="第二种方式添加自定义 pattern"></a>第二种方式添加自定义 pattern</h2><p>第二种方式就如同我们一开始使用的一样，在指定的 pattern 文件夹，添加即可比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 在 ./pattern/postfix 添加</span><br><span class="line">POSTFIX_QUEUEID [0-9A-F]&#123;10,11&#125;</span><br></pre></td></tr></table></figure></p>
<p>在实际匹配的时候写入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%&#123;POSTFIX_QUEUEID:queue_id&#125;</span><br></pre></td></tr></table></figure></p>
<p>使用的效果与第一种方式完全一致。</p>
<p>Logstash 的配置文件中可以指定 自定义 pattern 的位置，比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    patterns_dir =&gt; [&quot;./patterns&quot;]</span><br><span class="line">    match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;SYSLOGBASE&#125; %&#123;POSTFIX_QUEUEID:queue_id&#125;: %&#123;GREEDYDATA:syslog_message&#125;&quot; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>就是读取当前目录下的 patterns 文件。</p>
<h1 id="Grok-插件的一些配置项"><a href="#Grok-插件的一些配置项" class="headerlink" title="Grok 插件的一些配置项"></a>Grok 插件的一些配置项</h1><p>如果需要定制化 Grok 插件的一些匹配配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>配置项名称</th>
<th>匹配项类型</th>
<th>是否必须</th>
</tr>
</thead>
<tbody>
<tr>
<td>break_on_match</td>
<td>boolean</td>
<td>no</td>
</tr>
<tr>
<td>keep_empty_capture</td>
<td>boolean</td>
<td>no</td>
</tr>
<tr>
<td>match</td>
<td>hash</td>
<td>no</td>
</tr>
<tr>
<td>named_captures_only</td>
<td>boolean</td>
<td>no</td>
</tr>
<tr>
<td>overwrite</td>
<td>array</td>
<td>no</td>
</tr>
<tr>
<td>pattern_definitions</td>
<td>hash</td>
<td>no</td>
</tr>
<tr>
<td>patterns_dir</td>
<td>array</td>
<td>no</td>
</tr>
<tr>
<td>patterns_files_glob</td>
<td>string</td>
<td>no</td>
</tr>
<tr>
<td>tag_on_failure</td>
<td>array</td>
<td>no</td>
</tr>
<tr>
<td>tag_on_timeout</td>
<td>string</td>
<td>no</td>
</tr>
<tr>
<td>timeout_millis</td>
<td>number</td>
<td>no</td>
</tr>
</tbody>
</table>
</div>
<h2 id="named-captures-only"><a href="#named-captures-only" class="headerlink" title="named_captures_only"></a>named_captures_only</h2><p>默认值： <code>true</code><br>意义：当该值为 <code>true</code> 时，只会从数据中捕获有 <code>SEMANTIC</code> 的信息，其他的匹配项不存储。</p>
<h2 id="overwrite"><a href="#overwrite" class="headerlink" title="overwrite"></a>overwrite</h2><p>默认值：[]<br>意义：这个属性允许你在输出匹配结果之前，重写该匹配结果中的一项或多项，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 原始数据</span><br><span class="line">734742416494845952,Android,&quot;In trade, military and EVERYTHING else, it will be AMERICA FIRST! This will quickly lead to our ultimate goal: MAKE AMERICA GREAT AGAIN!&quot;,2016-05-23T13:46:57Z</span><br><span class="line"># 配置文件</span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    patterns_dir =&gt; [&quot;/root/custom-patterns.txt&quot;]</span><br><span class="line">    match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;ID:id&#125;,%&#123;DEVICE:device&#125;,%&#123;MESSAGE_BODY:message&#125;,%&#123;TIMESTAMP_ISO8601:timestamp&#125;&quot; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"># 结果1 没有overwrite选项</span><br><span class="line">&quot;message&quot;: [</span><br><span class="line">&quot;734742416494845952,Android,&quot;In trade, military and EVERYTHING else, it will be AMERICA FIRST! This will quickly lead to our ultimate goal: MAKE AMERICA GREAT AGAIN!&quot;,2016-05-23T13:46:57Z&quot;</span><br><span class="line">,</span><br><span class="line">&quot;&quot;In trade, military and EVERYTHING else, it will be AMERICA FIRST! This will quickly lead to our ultimate goal: MAKE AMERICA GREAT AGAIN!&quot;&quot;</span><br><span class="line">]，</span><br><span class="line">&quot;host&quot;: &quot;localhost&quot;,</span><br><span class="line">&quot;@timestamp&quot;: &quot;2018-10-21T01:09:04.089Z&quot;,</span><br><span class="line">&quot;device&quot;: &quot;Android&quot;,</span><br><span class="line">&quot;id&quot;: &quot;734742416494845952&quot;,</span><br><span class="line">&quot;timestamp&quot;: &quot;2016-05-23T13:46:57Z&quot;,</span><br><span class="line">&quot;path&quot;: &quot;/root/trump-test.csv&quot;,</span><br><span class="line">&quot;@version&quot;: &quot;1&quot;</span><br><span class="line"></span><br><span class="line"># 结果2 设置overwrite选项     overwrite =&gt; [&quot;message&quot;]</span><br><span class="line">&quot;message&quot;: &quot;In trade, military and EVERYTHING else, it will be AMERICA FIRST! This will quickly lead to our ultimate goal: MAKE AMERICA GREAT AGAIN!&quot;,</span><br><span class="line">&quot;host&quot;: &quot;localhost&quot;,</span><br><span class="line">&quot;@timestamp&quot;: &quot;2018-10-21T01:09:04.089Z&quot;,</span><br><span class="line">&quot;device&quot;: &quot;Android&quot;,</span><br><span class="line">&quot;id&quot;: &quot;734742416494845952&quot;,</span><br><span class="line">&quot;timestamp&quot;: &quot;2016-05-23T13:46:57Z&quot;,</span><br><span class="line">&quot;path&quot;: &quot;/root/trump-test.csv&quot;,</span><br><span class="line">&quot;@version&quot;: &quot;1&quot;</span><br></pre></td></tr></table></figure>
<p>可以看到，原来的 <code>message</code> 属性存储的是原始的输入信息，增加了 overwrite 选项后，则由匹配到的 message 信息将其替换。</p>
<h2 id="pattern-definitions"><a href="#pattern-definitions" class="headerlink" title="pattern_definitions"></a>pattern_definitions</h2><p>默认值：空<br>意义：可以再该项中，以键值对的方式输入 grok 表达式，也就是把patterns_dir 文件内的 pattern 写在这个属性里，比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pattern_definitions =&gt; &#123;</span><br><span class="line">     &quot;ID&quot; =&gt; &quot;\d+(?=,)&quot;</span><br><span class="line">     &quot;DEVICE&quot; =&gt; &quot;[a-zA-Z]+(?=,)&quot;</span><br><span class="line">     &quot;MESSAGE_BODY&quot; =&gt; &quot;.+(?=,2)&quot;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="timeout-millis"><a href="#timeout-millis" class="headerlink" title="timeout_millis"></a>timeout_millis</h2><p>默认值：30000<br>意义：在执行了所定义的时间后，终止正则匹配，设置为0则没有超时时间。</p>
<h2 id="add-tag"><a href="#add-tag" class="headerlink" title="add_tag"></a>add_tag</h2><p>默认值：[]<br>意义：在匹配成功后，增加一个或多个 tag 字段</p>
<h2 id="add-field"><a href="#add-field" class="headerlink" title="add_field"></a>add_field</h2><p>默认值：{}<br>意义：在匹配成功后，增加一个或多个 field、value对，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 输入</span><br><span class="line">依然是上文中使用的例子</span><br><span class="line"></span><br><span class="line"># Logstash 配置文件如下</span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    pattern_definitions =&gt; &#123;</span><br><span class="line">      &quot;ID&quot; =&gt; &quot;\d+(?=,)&quot;</span><br><span class="line">      &quot;DEVICE&quot; =&gt; &quot;[a-zA-Z]+(?=,)&quot;</span><br><span class="line">      &quot;MESSAGE_BODY&quot; =&gt; &quot;.+(?=,2)&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    #patterns_dir =&gt; [&quot;/root/custom-patterns.txt&quot;]</span><br><span class="line">    match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;ID:id&#125;,%&#123;DEVICE:device&#125;,%&#123;MESSAGE_BODY:message&#125;,%&#123;TIMESTAMP_ISO8601:timestamp&#125;&quot; &#125;</span><br><span class="line">    overwrite =&gt; [&quot;message&quot;]</span><br><span class="line">    add_tag =&gt; [ &quot;test_%&#123;id&#125;&quot;]</span><br><span class="line">    add_field =&gt; &#123;</span><br><span class="line">      &quot;test_%&#123;timestamp&#125;&quot; =&gt; &quot;test&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    remove_field =&gt; [&quot;device&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 结果如下</span><br><span class="line">&quot;host&quot;: &quot;localhost&quot;,</span><br><span class="line">&quot;timestamp&quot;: &quot;2016-05-23T13:46:57Z&quot;,</span><br><span class="line">&quot;id&quot;: &quot;734742416494845952&quot;,</span><br><span class="line">&quot;tags&quot;: [</span><br><span class="line">&quot;test_734742416494845952&quot;</span><br><span class="line">],</span><br><span class="line">&quot;@version&quot;: &quot;1&quot;,</span><br><span class="line">&quot;@timestamp&quot;: &quot;2018-10-21T01:47:31.552Z&quot;,</span><br><span class="line">&quot;message&quot;: &quot;In trade, military and EVERYTHING else, it will be AMERICA FIRST! This will quickly lead to our ultimate goal: MAKE AMERICA GREAT AGAIN!&quot;,</span><br><span class="line">&quot;path&quot;: &quot;/root/trump-test.csv&quot;,</span><br><span class="line">&quot;test_2016-05-23T13:46:57Z&quot;: &quot;test&quot;</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html" target="_blank" rel="noopener">grok manual</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2018/10/25/20181025LogstashGrokOverview/">http://littleji.com/2018/10/25/20181025LogstashGrokOverview/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-10-21T16:00:00.000Z"><a href="/2018/10/22/20181022ProbabilisticGraphicalModels1/">2018-10-22</a></time>
      
      
  
    <h1 class="title"><a href="/2018/10/22/20181022ProbabilisticGraphicalModels1/">概率图模型之一</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="概率图概述"><a href="#概率图概述" class="headerlink" title="概率图概述"></a>概率图概述</h1><p>概率图模型使用图的方法来表示概率分布，在该模型中，结点表示变量，节点之间的边表示变量之间的概略关系。</p>
<p>根据图模型中的边是否有向，将概率图的代表模型如下：<br>￼<br><img src="https://user-images.githubusercontent.com/7655877/47288361-5f420c00-d628-11e8-9ad1-555e47176c16.png" alt="image"></p>
<h2 id="无向图模型和有向图模型的区别"><a href="#无向图模型和有向图模型的区别" class="headerlink" title="无向图模型和有向图模型的区别"></a>无向图模型和有向图模型的区别</h2><p>这里要说明的是,采用有向图的贝叶斯网络的”有向”表示的是依存关系,具有因果推断,即A-&gt;B-&gt;C,反之则不可.<br>而无向图的代表马尔科夫网络,采用的无向图</p>
<p>各个图模型的演变关系如下：</p>
<p><img src="https://user-images.githubusercontent.com/7655877/47288381-7a148080-d628-11e8-836a-fbe66c555d01.png" alt="image"></p>
<p>其中横向，由点到线（序列结构），最终到面。<br>纵向则是在一定的条件下生成式模型转换为判别式模型。<br>下面假设有观测序列x，状态序列y，并依次来说明生成式与判别式模型的区别。</p>
<h2 id="生成式模型"><a href="#生成式模型" class="headerlink" title="生成式模型"></a>生成式模型</h2><p>生成式模型的定义是：“状态（输出）序列y按照一定的规律生成观测（输入）序列x”。生成式模型的本质是对于联合概率分布p(x,y)进行建模，并根据生成概率最大的生成序列来获取y。</p>
<p>这类模型中，一般有严格的独立性假设，模型变量之间的关系清楚，处理单类问题时较为灵活。</p>
<p>弱点是模型的推导与学习较为复杂。</p>
<p>主要的模型有：n-gram，HMM、朴素贝叶斯、概率上下文无关文法。</p>
<h2 id="判别式模型"><a href="#判别式模型" class="headerlink" title="判别式模型"></a>判别式模型</h2><p>判别式模型的定义是：“状态（输出）序列y是由观测序列（输入）所决定的。”判别式模型的本质是对后验概率p（y|x）进行建模，优点是处理多累问题或分辨某一类与其他类差异时更为灵活，模型构造简单。</p>
<p>缺点是模型的描述能力有限，变量之间的关系不清楚。大多数模型都是有监督学习，不能很好地扩展为无监督学习。</p>
<p>主要模型有：最大熵模型、最大熵马尔科夫模型、支持向量机、条件随机场、感知机。</p>
<h1 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h1><p>贝叶斯网络也称为belief networks，是一种基于概率推理的数学模型，理论基础为贝叶斯公式。</p>
<p>贝叶斯网络形式上是一个有向无环图（DAG directed acyclic graph），结点表示随机变量，结点之间的边表示条件依存关系，箭头出发的节点为父节点，箭头到达的节点为子节点，子节点依存于父节点。</p>
<p>如果两个节点没有连接关系，表示两个随机变量能够在某些特定情况下条件独立。</p>
<h2 id="构造贝叶斯网络"><a href="#构造贝叶斯网络" class="headerlink" title="构造贝叶斯网络"></a>构造贝叶斯网络</h2><p>构造贝叶斯网络是一个复杂的任务，其主要有三个方面的问题：表示、推断、学习</p>
<h3 id="表示"><a href="#表示" class="headerlink" title="表示"></a>表示</h3><p>在简单某一随机变量的组合上<br><img src="https://user-images.githubusercontent.com/7655877/47356431-a4cd0a80-d6f6-11e8-9304-e72390800815.png" alt="image"><br>即便是随机变量只有两个取值，那么联合概率分布P需要对 2^n 种不同取值下的概率情况进行说明，然而这件事的计算代价非常高。</p>
<h3 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h3><p>由于贝叶斯网络是变量以及关系的完整模型，那么在观测到某些变量变化的时候就需要使用一些推断方法，来得知另一些变量子集的变化。<br>概率推理：在已知某些证据的情况下，计算变量的后验分布的过程。<br>常用的精确推理方法有两种</p>
<ul>
<li>变量消除法 variable elimination</li>
<li>团树法 clique tree</li>
</ul>
<p>常用的近似推理</p>
<ul>
<li>重要性抽样 importance sampling</li>
<li>随机马尔科夫链蒙特卡洛模拟法 Markov chain Monte Carlo，MCMC</li>
<li>循环信念传播法 loopy belief propagation</li>
<li>泛化信念传播法 generalized belief propagation</li>
</ul>
<h3 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h3><p>参数学习的目的就是得知各个变量结点之间相互依存的强度。<br>比如给定某个节点X，需要计算 P(X|父节点1，父节点2…)，这些概率分布可以是任意形式，通常是离散分布与高斯分布。</p>
<p>常用的参数学习方法包括：</p>
<ul>
<li>最大似然估计 MLE</li>
<li>最大后验概率 MAP</li>
<li>期望最大 EM</li>
<li>贝叶斯估计方法，贝叶斯图模型中，使用较多的是该种方法</li>
</ul>
<p>除了参数学习，还需要结构学习来学习各个变量之间的图关系，简单的贝叶斯可以由有经验的专家来构造，但一般情况下人工构造一个贝叶斯网络的结构几乎不可能，所以自动结构学习是一项颇具挑战的任务。</p>
<h2 id="MLE-MAP-贝叶斯估计之间的关系"><a href="#MLE-MAP-贝叶斯估计之间的关系" class="headerlink" title="MLE MAP 贝叶斯估计之间的关系"></a>MLE MAP 贝叶斯估计之间的关系</h2><p>其中，最大似然估计（MLE）是频率派的代表，贝叶斯估计（Bayes）是贝叶斯派的代表，最大后验估计是频率派和贝叶斯派的合成，是一种规则化后的最大似然估计。</p>
<p>一般来说，在我们对于先验概率一无所知时，只能假设每种猜测的先验概率是均等的（其实这也是人类经验的结果），这个时候就只有用最大似然了，但不能忘记一个重要的前提是所有的采样都是独立同分布。</p>
<p>如果我们有足够的自信，训练集中的样本分布的确很接近真实的情况，这时就应该用贝叶斯方法。贝叶斯学派强调的是“靠谱的先验概率”。所以说贝叶斯学派的适用范围更广，关键要先验概率靠谱，而频率学派有效的前提也是他们的先验概率同样是经验统计的结果。但也说明了贝叶斯计算要更为复杂，因为需要选择一个常用分布，并确定一个初始参数集作为先验分布。</p>
<p>再者，MAP与MLE最大区别是MAP中加入了模型参数本身的概率分布，即是否考虑了先验知识。或者说。MLE中认为模型参数本身的概率的是均匀的，即该概率为一个固定值。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>统计自然语言处理（第2版）<br><a href="https://blog.csdn.net/Mr_tyting/article/details/62882162?utm_source=blogxgwz1" target="_blank" rel="noopener">1</a><br><a href="https://blog.csdn.net/juanjuan1314/article/details/78189527" target="_blank" rel="noopener">2</a><br><a href="https://zhuanlan.zhihu.com/p/32568242" target="_blank" rel="noopener">3</a><br><a href="https://zhuanlan.zhihu.com/p/32616870" target="_blank" rel="noopener">4</a><br><a href="https://blog.csdn.net/guohecang/article/details/52313046" target="_blank" rel="noopener">5</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2018/10/22/20181022ProbabilisticGraphicalModels1/">http://littleji.com/2018/10/22/20181022ProbabilisticGraphicalModels1/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2018-10-15T16:00:00.000Z"><a href="/2018/10/16/20181016FormalLanguageGenerality/">2018-10-16</a></time>
      
      
  
    <h1 class="title"><a href="/2018/10/16/20181016FormalLanguageGenerality/">形式语法概述</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>形式语言作为精确描述语言的工具，广泛的应用在机器翻译等自然语言处理的领域</p>
<p>描述语言的常用三个方法：</p>
<ol>
<li>穷举法，对于句子数目有限的语言可以使用这种方法将所有的句子进行枚举，从而定义。</li>
<li>文法，可以理解为我们在英文、中文中学习的语法，通过严格定义规则，来生成合法的句子，<strong>重点</strong>在于生成句子。</li>
<li>自动机，不同于文法，自动机<strong>更偏向</strong>与对于输入句子的合法性检测，从而区分哪些是语言中的句子，哪些不是。</li>
</ol>
<p>文法与自动机二者皆有所长，一定条件下可以相互转换。</p>
<h2 id="符号与符号串"><a href="#符号与符号串" class="headerlink" title="符号与符号串"></a>符号与符号串</h2><h3 id="字母表，符号集"><a href="#字母表，符号集" class="headerlink" title="字母表，符号集"></a>字母表，符号集</h3><p>字母或者符号的又穷非空集合。<br>例：汉语字母表为汉字、数字、标点符号。</p>
<h3 id="符号串"><a href="#符号串" class="headerlink" title="符号串"></a>符号串</h3><p>字母表中的符号组成的任何有穷序列。<br>例：有符号集<br><img src="https://user-images.githubusercontent.com/7655877/46996847-9314c180-d150-11e8-9367-fca90bcc4c86.png" alt="image"><br>符号串为：0，1，01，10，11，00</p>
<h3 id="空字符串"><a href="#空字符串" class="headerlink" title="空字符串"></a>空字符串</h3><p><img src="https://user-images.githubusercontent.com/7655877/46997037-32d24f80-d151-11e8-8fe0-88a86046bd19.png" alt="image">表示，长度为0</p>
<h3 id="符号串的头、尾、固有头、固有尾"><a href="#符号串的头、尾、固有头、固有尾" class="headerlink" title="符号串的头、尾、固有头、固有尾"></a>符号串的头、尾、固有头、固有尾</h3><p>例：如果z=abc，则<br>z的头为：<img src="https://user-images.githubusercontent.com/7655877/46997037-32d24f80-d151-11e8-8fe0-88a86046bd19.png" alt="image">，a，ab，abc<br>z的尾为：<img src="https://user-images.githubusercontent.com/7655877/46997037-32d24f80-d151-11e8-8fe0-88a86046bd19.png" alt="image">，c，bc，cba<br>z的固有头：<img src="https://user-images.githubusercontent.com/7655877/46997037-32d24f80-d151-11e8-8fe0-88a86046bd19.png" alt="image">，a，ab<br>z的固有尾：<img src="https://user-images.githubusercontent.com/7655877/46997037-32d24f80-d151-11e8-8fe0-88a86046bd19.png" alt="image">，c，bc</p>
<h3 id="符号串连接"><a href="#符号串连接" class="headerlink" title="符号串连接"></a>符号串连接</h3><p>有符号串x，y，连接xy指y符号串写在x符号串之后<br>例：x=ab，y=cd，xy=abcd</p>
<h3 id="符号串的方幂"><a href="#符号串的方幂" class="headerlink" title="符号串的方幂"></a>符号串的方幂</h3><p>有符号串x，z=xxxx….，称z为x的方幂，记为<img src="https://user-images.githubusercontent.com/7655877/46997369-1aaf0000-d152-11e8-9fe2-15669611d07f.png" alt="image"><br>且<br><img src="https://user-images.githubusercontent.com/7655877/46997436-44682700-d152-11e8-81b0-2e666a016eda.png" alt="image"></p>
<h3 id="符号串的相乘"><a href="#符号串的相乘" class="headerlink" title="符号串的相乘"></a>符号串的相乘</h3><p>有符号串<img src="https://user-images.githubusercontent.com/7655877/46997662-d6702f80-d152-11e8-8624-c2bd89787f26.png" alt="image"><br>例：有A={a,b} B={c,d},则AB={ac,ad,bc,bd}</p>
<h3 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h3><p>字母表<img src="https://user-images.githubusercontent.com/7655877/46999201-98c1d580-d157-11e8-9f72-e44129b97319.png" alt="image">上所有又穷长的字符串的集合用<img src="https://user-images.githubusercontent.com/7655877/46999213-a0817a00-d157-11e8-9da5-49f7d12b8df4.png" alt="image"><br>来表示，其中正闭包<img src="https://user-images.githubusercontent.com/7655877/46999164-7af47080-d157-11e8-82d2-88a9f3c888a5.png" alt="image">不包括空集</p>
<h2 id="文法和语言的形式定义"><a href="#文法和语言的形式定义" class="headerlink" title="文法和语言的形式定义"></a>文法和语言的形式定义</h2><h3 id="规则"><a href="#规则" class="headerlink" title="规则"></a>规则</h3><p>也称为产生式：<br><img src="https://user-images.githubusercontent.com/7655877/46999289-d6bef980-d157-11e8-8607-e0407655604f.png" alt="image"><br>其中，<img src="https://user-images.githubusercontent.com/7655877/46999328-f5bd8b80-d157-11e8-9468-08b46507690a.png" alt="image"></p>
<h3 id="文法"><a href="#文法" class="headerlink" title="文法"></a>文法</h3><p>定义：G定义为四元组<img src="https://user-images.githubusercontent.com/7655877/46999397-28678400-d158-11e8-9dc1-bfbdfac5462e.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7655877/46999410-30bfbf00-d158-11e8-8c83-d1f35dc4d3d8.png" alt="image"></p>
<h3 id="句型"><a href="#句型" class="headerlink" title="句型"></a>句型</h3><p><img src="https://user-images.githubusercontent.com/7655877/46999714-0a4e5380-d159-11e8-8a22-4cac5e52f986.png" alt="image"><br>句子一定是句型，句型不一定是句子</p>
<h3 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h3><p><img src="https://user-images.githubusercontent.com/7655877/46999780-336ee400-d159-11e8-8067-05c802ea65bb.png" alt="image">记为L(G)</p>
<h2 id="文法类型"><a href="#文法类型" class="headerlink" title="文法类型"></a>文法类型</h2><p><img src="https://user-images.githubusercontent.com/7655877/47000450-e2f88600-d15a-11e8-9f6f-2c76109410e1.png" alt="image"></p>
<h3 id="上下文无关文法"><a href="#上下文无关文法" class="headerlink" title="上下文无关文法"></a>上下文无关文法</h3><p><img src="https://user-images.githubusercontent.com/7655877/47001280-aa59ac00-d15c-11e8-89ba-c9a2200b1f17.png" alt="image"><br>重点是在规则的左部只有一个非终结字符</p>
<h3 id="正规文法"><a href="#正规文法" class="headerlink" title="正规文法"></a>正规文法</h3><p><img src="https://user-images.githubusercontent.com/7655877/47001685-a0847880-d15d-11e8-87f0-fe81ca258def.png" alt="image"></p>
<h3 id="语法树"><a href="#语法树" class="headerlink" title="语法树"></a>语法树</h3><p>语法树表明了推倒过程中使用了什么样的产生式和用到了哪些非终结字符，并不表明顺序。<br>例：有下图<br><img src="https://user-images.githubusercontent.com/7655877/47007060-f9f2a480-d169-11e8-8f60-9e1edf81a9ad.png" alt="image"><br>构造aabbaa的语法树，步骤如下<br><img src="https://user-images.githubusercontent.com/7655877/47007084-07a82a00-d16a-11e8-9aaa-8fc4df075698.png" alt="image"></p>
<h3 id="文法的二义性"><a href="#文法的二义性" class="headerlink" title="文法的二义性"></a>文法的二义性</h3><p><img src="https://user-images.githubusercontent.com/7655877/47002932-c0696b80-d160-11e8-9759-84258745460f.png" alt="image"></p>
<h2 id="句型分析"><a href="#句型分析" class="headerlink" title="句型分析"></a>句型分析</h2><p>识别一个符号串是否为某文法的句型是整个推导的构建过程</p>
<h3 id="自上而下分析法"><a href="#自上而下分析法" class="headerlink" title="自上而下分析法"></a>自上而下分析法</h3><p>由非终结字符串推导至终结字符串，并查看该终结字符串是否匹配</p>
<h3 id="自下而上分析法"><a href="#自下而上分析法" class="headerlink" title="自下而上分析法"></a>自下而上分析法</h3><p>有终结字符串进行规约，最终生成的非终结字符串，是否符合规则</p>
<h3 id="回溯法"><a href="#回溯法" class="headerlink" title="回溯法"></a>回溯法</h3><p><img src="https://user-images.githubusercontent.com/7655877/47004899-2657f200-d165-11e8-94fb-5463a90dd1f7.png" alt="image"><br>这时候使用回溯法进行计算</p>
<h2 id="简化文法"><a href="#简化文法" class="headerlink" title="简化文法"></a>简化文法</h2><p>主要是去除规则中的两种类型的规则，有害规则和多与规则</p>
<h3 id="有害规则"><a href="#有害规则" class="headerlink" title="有害规则"></a>有害规则</h3><p>例如：<br>U-&gt;U这种产生式，会引起文法的二义性。<br>多余规则：指文法中任何句子的推倒都不会用到的规则，主要有两种：</p>
<ol>
<li>非终结字符不在任何规则的右部出现。</li>
<li>非终结字符无法推出终结符号串</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>宗成庆. 统计自然语言处理[M]. 清华大学出版社, 2008.<br><a href="http://ccl.pku.edu.cn/ALCourse/Compiling/" target="_blank" rel="noopener">北京大学编译原理课程</a></p>
<hr>
<p>版权声明:本文由littleji.com创作并发表,转载请注明作者及出处,欢迎关注公众号:littleji_com<br><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">本文遵守CC BY0SA 4.0</a><br>if you have any questions, please leave a message behind or give an <a href="https://github.com/littleji/littleji.github.io/issues" target="_blank" rel="noopener">issue</a></p>
<p>本文链接为：<a href="http://littleji.com/2018/10/16/20181016FormalLanguageGenerality/">http://littleji.com/2018/10/16/20181016FormalLanguageGenerality/</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>






<nav id="pagination">
  
  
    <a href="/page/2/" class="alignright next">下一页</a>
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//baidu.com" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:littleji.com">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">分类</h3>
  <ul class="entry">
  
    <li><a href="/categories/2019-03/">2019-03</a><small>1</small></li>
  
    <li><a href="/categories/2019-02/">2019-02</a><small>2</small></li>
  
    <li><a href="/categories/2018-12/">2018-12</a><small>3</small></li>
  
    <li><a href="/categories/2018-11/">2018-11</a><small>1</small></li>
  
    <li><a href="/categories/2018-10/">2018-10</a><small>3</small></li>
  
    <li><a href="/categories/2018-09/">2018-09</a><small>2</small></li>
  
    <li><a href="/categories/2018-02/">2018-02</a><small>1</small></li>
  
    <li><a href="/categories/2018-01/">2018-01</a><small>2</small></li>
  
    <li><a href="/categories/2017-05/">2017-05</a><small>1</small></li>
  
    <li><a href="/categories/2016-12/">2016-12</a><small>4</small></li>
  
    <li><a href="/categories/2016-11/">2016-11</a><small>4</small></li>
  
    <li><a href="/categories/2016-05/">2016-05</a><small>2</small></li>
  
    <li><a href="/categories/2016-04/">2016-04</a><small>1</small></li>
  
  </ul>
</div>



  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithm/">Algorithm</a><small>1</small></li>
  
    <li><a href="/tags/BookList/">BookList</a><small>1</small></li>
  
    <li><a href="/tags/DataMining/">DataMining</a><small>1</small></li>
  
    <li><a href="/tags/DataStructure/">DataStructure</a><small>1</small></li>
  
    <li><a href="/tags/DistributedComputation/">DistributedComputation</a><small>3</small></li>
  
    <li><a href="/tags/ELK/">ELK</a><small>3</small></li>
  
    <li><a href="/tags/ElasticSearch/">ElasticSearch</a><small>2</small></li>
  
    <li><a href="/tags/Error/">Error</a><small>1</small></li>
  
    <li><a href="/tags/Git/">Git</a><small>1</small></li>
  
    <li><a href="/tags/Golang/">Golang</a><small>1</small></li>
  
    <li><a href="/tags/Graph/">Graph</a><small>1</small></li>
  
    <li><a href="/tags/Java/">Java</a><small>2</small></li>
  
    <li><a href="/tags/Linux/">Linux</a><small>2</small></li>
  
    <li><a href="/tags/Logstash/">Logstash</a><small>2</small></li>
  
    <li><a href="/tags/MachineLearning/">MachineLearning</a><small>5</small></li>
  
    <li><a href="/tags/MySQL/">MySQL</a><small>1</small></li>
  
    <li><a href="/tags/Mybatis/">Mybatis</a><small>1</small></li>
  
    <li><a href="/tags/NLP/">NLP</a><small>4</small></li>
  
    <li><a href="/tags/Navicat/">Navicat</a><small>1</small></li>
  
    <li><a href="/tags/ORM/">ORM</a><small>1</small></li>
  
    <li><a href="/tags/Programmer/">Programmer</a><small>1</small></li>
  
    <li><a href="/tags/Programming/">Programming</a><small>5</small></li>
  
    <li><a href="/tags/Rpc/">Rpc</a><small>2</small></li>
  
    <li><a href="/tags/Security/">Security</a><small>1</small></li>
  
    <li><a href="/tags/SiteManagement/">SiteManagement</a><small>1</small></li>
  
    <li><a href="/tags/SoftwareEngineering/">SoftwareEngineering</a><small>2</small></li>
  
    <li><a href="/tags/Translate/">Translate</a><small>2</small></li>
  
    <li><a href="/tags/jsonrpc4j/">jsonrpc4j</a><small>2</small></li>
  
    <li><a href="/tags/life/">life</a><small>1</small></li>
  
    <li><a href="/tags/信息安全/">信息安全</a><small>1</small></li>
  
    <li><a href="/tags/区块链/">区块链</a><small>1</small></li>
  
    <li><a href="/tags/图/">图</a><small>1</small></li>
  
    <li><a href="/tags/思考/">思考</a><small>1</small></li>
  
    <li><a href="/tags/数据挖掘/">数据挖掘</a><small>1</small></li>
  
    <li><a href="/tags/数据结构/">数据结构</a><small>1</small></li>
  
    <li><a href="/tags/框架/">框架</a><small>1</small></li>
  
    <li><a href="/tags/算法/">算法</a><small>1</small></li>
  
    <li><a href="/tags/自然语言处理/">自然语言处理</a><small>1</small></li>
  
    <li><a href="/tags/计算机语言/">计算机语言</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">最新文章</h3>
  <ul class="entry">
    
      <li>
        <a href="/2019/03/03/20190303DataWhaleNLPTask1/">windows下TensorFlow安装与imdb文本分类</a>
      </li>
    
      <li>
        <a href="/2019/02/19/20190219HandoverMemoList/">软件项目交接清单</a>
      </li>
    
      <li>
        <a href="/2019/02/12/20190212OptimizeTheDevelopmentProcess/">用gitlab来优化我们的软件开发流程</a>
      </li>
    
      <li>
        <a href="/2018/12/25/20181225MLReview3/">七天算法梳理之决策树</a>
      </li>
    
      <li>
        <a href="/2018/12/22/20181222MLReview2/">七天算法梳理之逻辑回归</a>
      </li>
    
  </ul>
</div>


  
<div class="widget links">
  <h3 class="title">友链</h3>
  <ul class="entry">
    
      <li><a href="http://www.xiaobaidonghui.cn">小白</a></li>
    
  </ul>
</div>


</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2019 littleji
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'littleji';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
